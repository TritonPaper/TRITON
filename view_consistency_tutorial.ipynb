{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This section is a bunch of notes that I'm writing for myself. You needn't read them. Skip to the introduction.\n",
    "\n",
    "# TODO: This will be the delegated notebook!\n",
    "\n",
    "\n",
    "# FRI JAN 8 TODO:\n",
    "# TODO: Create symlinks in this directory for the dataset so that it can be exported to other people more easily\n",
    "# TODO: Clean up the names of things in this notebook\n",
    "# TODO: Move all these functions to other files and create a delegation notebook to test them\n",
    "# TODO: Create the standard deviation measurement from multiple translations, and create a visualization for that deviation in this notebook (with some tests, of course)\n",
    "# class ViewConsistencyVarianceLoss: def __init__(self, tex_width, tex_height, num_labels, pyramid_weights=[1,1,1])\n",
    "#    def forward(self, scene_uvs, scene_translations)\n",
    "# TODO: Smooth moving cube blender animation for demos\n",
    "# FOR EXPERIMENT: With pure simulated data, like textured cube in blender that moves around, we could use mean squared error for measuring how good each method is!\n",
    "# TODO: Figure out why the table is so blurry in the naive reconstructions. Is this because the MUNIT is randomly shifting the result image? It seems to be a discerete blur, in that a few shifts are averaged together...\n",
    "# NOTE: it might be beneficial to use multiple values of recovery_resolution in the view consistency loss; because that way it can criticize both high and low detail scales. This can be done with multiple ViewConsistencyLoss objects; perhaps aggregated into a MultiScaleViewConsistencyLoss(nn.Module) class. \n",
    "# NOTE: Uses of this might be for: reinforcement learning with multiple cameras, mobile robots, reinforcement learning with some temporal memory beetween frames and/or using optical flow. Might also be useful for data augmentation for image segmentation and object detection tasks?\n",
    "# TODO: Once we get the thing working, can we then use the vid2vid to bake textures onto objects?\n",
    "\n",
    "#TODO: Try using the crummy recovered textures from averaging the cyclegan outputs as the initial texture instead of random noise. Also try using a network that returns its own input as an initial neural networkk.\n",
    "\n",
    "\n",
    "#IDEA: There's also a texture that gets evolved. The texture is pushed to match the output of teh translations, but the translations are only pushed to match the hue of that texture. That way it doesn't hold back the translation network **too** much but shuold remain non-blurry. Or better yet, the translation network is only responsible for shading and the texture does the rest...\n",
    "\n",
    "#HOW TO INTEGRATE:\n",
    "#    turn the learned-neural-rendered-image-projection thing into a DataLoader class, and substitute that in for the current dataloader for the MUNIT algorihtm.\n",
    "#    then, just add the consistency loss. Do\n",
    "\n",
    "\n",
    "#RENAMINGS: textures, weights becomes texture_pack, weight_pack\n",
    "#RENAMINGS: num_labels becomes num_textures\n",
    "#TODO: Define what a texture and scene are, with pictures ov UV scenes etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from source.scene_reader import extract_scene_uvs_and_scene_labels\n",
    "from source.projector    import colorized_scene_labels\n",
    "from source.projector    import project_textures\n",
    "from source.unprojector  import unproject_translations, unproject_translations_individually\n",
    "\n",
    "from rp import *\n",
    "import torch\n",
    "import icecream\n",
    "\n",
    "#Install packages if needed:\n",
    "pip_import('einops');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "#Make the pixels of Jupyter-displayed images \n",
    "# use nearest-neigbor interpolation\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "img {\n",
    "  image-rendering: auto;\n",
    "  image-rendering: crisp-edges;\n",
    "  image-rendering: pixelated;\n",
    "}\n",
    "</style>\n",
    "\"\"\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\") #Uses less VRAM so we can train while running this notebook\n",
    "\n",
    "icecream.ic(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "    if isinstance(images,torch.Tensor):\n",
    "        images=as_numpy_images(images)\n",
    "    display_image(tiled_images(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_numpy_image(image):\n",
    "    if isinstance(image,np.ndarray):\n",
    "        return image.copy()\n",
    "    else:\n",
    "        return as_numpy_images(image.unsqueeze(0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images,size,interp='bilinear'):\n",
    "    return [cv_resize_image(image,size,interp) for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The goal of this tutorial is to show you how some of the functions in this project are used, giving you\")\n",
    "print(\"a more visual intuition for this project as a whole\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"This is a photo: the target domain is an alphabet block\")\n",
    "photo_image=as_rgb_image(load_image(random_element(get_all_files('datasets/alphacube/photos'))))\n",
    "icecream.ic(photo_image.shape)\n",
    "display_image(photo_image)\n",
    "\n",
    "print(\"This is a 'Scene': A picture of a 3d model's UV map (red/green), and blue 'label' channel indicating whats what\")\n",
    "scene_image=as_rgb_image(load_image(random_element(get_all_files('datasets/alphacube/scenes'))))\n",
    "icecream.ic(scene_image.shape)\n",
    "display_image(scene_image)\n",
    "\n",
    "print(\"This is an example of a 'Texture', an image that gets applied to UV maps for a particular label. \")\n",
    "print(\"This particular texture gets applied to the alphabet cube. Its a bit blurry because it was recovered from data.\")\n",
    "print(\"Note that in my code, we use square textures. This is an arbitrary choice; they don't have to be.\")\n",
    "display_image(load_image('assets/texture_example.png'))\n",
    "\n",
    "print(\"For reference, here are a few more photos so you can gauge the quality of the outputs:\")\n",
    "display_images(load_images(random_batch(get_all_files('datasets/alphacube/photos'),16)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Projection / Unprojection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_paths=get_all_files('datasets/alphacube/scenes',sort_by='number')\n",
    "image_paths=image_paths[:16] #For the previews, limit the number of samples. It makes the .ipynb files smaller.\n",
    "cube_models=load_images(image_paths,show_progress=True,use_cache=True)\n",
    "cube_models=[as_float_image(cube_model) for cube_model in cube_models]\n",
    "cube_models=as_numpy_array(cube_models)\n",
    "print(\"A random cube model:\")\n",
    "display_images(cube_models[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stone='https://www.filterforge.com/filters/12449.jpg'\n",
    "tiles='https://filterforge.com/filters/10857-v4.jpg'\n",
    "wood ='https://filterforge.com/filters/8892.jpg'\n",
    "paved='https://filterforge.com/filters/14157.jpg'\n",
    "metal='https://www.filterforge.com/filters/1375.jpg'\n",
    "gears='https://www.filterforge.com/filters/8624.jpg'\n",
    "walls='https://www.filterforge.com/filters/15245.jpg'\n",
    "grass='https://www.filterforge.com/filters/11635.jpg'\n",
    "china='https://www.filterforge.com/filters/9935.jpg'\n",
    "\n",
    "\n",
    "#Go ahead and modify this notebook here: choose your favorite two textures!\n",
    "#The first one goes to the cube, and the second one goes to the table.\n",
    "albedo       =china\n",
    "second_albedo=wood\n",
    "\n",
    "albedo       =load_image(albedo       ,use_cache=True)\n",
    "second_albedo=load_image(second_albedo,use_cache=True)\n",
    "\n",
    "\n",
    "#Display the images:\n",
    "ims=load_images([stone,tiles,wood,paved,metal,gears,walls,grass,china],use_cache=True)\n",
    "ims=resize_images(ims,.25)\n",
    "ims=labeled_images(ims,'stone,tiles,wood,paved,metal,gears,walls,grass,china'.split(','))\n",
    "ims=tiled_images(ims)\n",
    "print(\"Texture options:\")\n",
    "display_image(ims)\n",
    "\n",
    "print(\"Albedo Map:\")\n",
    "display_image(albedo)\n",
    "icecream.ic(albedo.shape)\n",
    "\n",
    "print(\"Second Albedo Map:\")\n",
    "display_image(second_albedo)\n",
    "icecream.ic(second_albedo.shape)\n",
    "\n",
    "\n",
    "#Create the torch tensors:\n",
    "torch_cube_models=as_torch_images(cube_models).to(device)\n",
    "\n",
    "torch_albedo       =torch.tensor(albedo       ).to(device).permute(2,0,1)/255\n",
    "torch_second_albedo=torch.tensor(second_albedo).to(device).permute(2,0,1)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Important Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene_uvs, scene_labels = extract_scene_uvs_and_scene_labels(torch_cube_models,[0,255])\n",
    "\n",
    "icecream.ic(scene_labels.flatten().unique())\n",
    "\n",
    "icecream.ic(torch_cube_models.shape,\n",
    "            scene_uvs        .shape,\n",
    "            scene_labels     .shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Colorized with arbitrary colors, such as blue and pink...\")\n",
    "colorized_labels = colorized_scene_labels(scene_labels, torch.Tensor([[1,0,.5],[0,.25,.5]]))\n",
    "display_images(colorized_labels[:4])\n",
    "\n",
    "print(\"Colorized with more arbitrary colors, such as black and green...\")\n",
    "colorized_labels = colorized_scene_labels(scene_labels, torch.Tensor([[0,1,0],[0,0,0]]))\n",
    "display_images(colorized_labels[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Demo 1: Albedo and Second Albedo (Arbitrary textures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "textures=torch.stack((torch_albedo, torch_second_albedo))\n",
    "\n",
    "icecream.ic(textures.shape)\n",
    "\n",
    "scene_projections = project_textures(scene_uvs, scene_labels, textures)\n",
    "print(\"Rendered images from torch: should look identical to the previous animation on every frame\")\n",
    "display_images(as_numpy_images(scene_projections[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unprojection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Demo 1: Albedo and Second Albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels=len(textures)\n",
    "recovery_resolution=1024\n",
    "# recovery_resolution=512\n",
    "# recovery_resolution=256\n",
    "recovered_textures, _ = unproject_translations(scene_projections                ,\n",
    "                                               scene_uvs                        ,\n",
    "                                               scene_labels                     ,\n",
    "                                               num_labels                       ,\n",
    "                                               output_height=recovery_resolution,\n",
    "                                               output_width =recovery_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unprojection mean:\")\n",
    "display_images(recovered_textures)\n",
    "w=torch.stack((_,_,_),dim=1)\n",
    "w=w/w.max(dim=1,keepdim=True)[0].max(dim=2,keepdim=True)[0].max(dim=3,keepdim=True)[0]\n",
    "print(\"Unprojection weights:\")\n",
    "display_images(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[]\n",
    "\n",
    "for scene_uv, scene_label, scene_projection, cube_model in zip(scene_uvs, scene_labels, scene_projections, cube_models):\n",
    "    # recovery_resolution=1024\n",
    "    recovery_resolution=512\n",
    "    # recovery_resolution=256\n",
    "    recovered_textures, _ = unproject_translations(scene_projection[None]           ,\n",
    "                                                   scene_uv        [None]           ,\n",
    "                                                   scene_label     [None]           ,\n",
    "                                                   num_labels                       ,\n",
    "                                                   output_height=recovery_resolution,\n",
    "                                                   output_width =recovery_resolution)\n",
    "    \n",
    "    scene_projection   = as_numpy_image (scene_projection  )\n",
    "    recovered_textures = as_numpy_images(recovered_textures)\n",
    "    \n",
    "    scene_width = get_image_width(scene_projection)\n",
    "    assert get_image_width(cube_model) == scene_width\n",
    "    \n",
    "    scene_stuff = [scene_projection, cube_model]\n",
    "    scene_stuff = resize_images (scene_stuff, recovery_resolution/scene_width          )\n",
    "    scene_stuff = labeled_images(scene_stuff, ['Scene Projection', 'UV Map and Labels'])\n",
    "    \n",
    "    recovered_textures = labeled_images(recovered_textures, ['Recovered Albedo','Recovered Second Albedo'])\n",
    "    \n",
    "    frame = grid_concatenated_images([recovered_textures, scene_stuff])\n",
    "    \n",
    "    frames.append(frame)\n",
    "\n",
    "display_image_slideshow(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Demo 2: Unprojecting Naive Image Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_data='./assets/naive_translation_samples_nonrandom.png'       ;num_samples=16\n",
    "naive_data='./assets/naive_translation_samples_nonrandom_nerfed.png';num_samples=32 # <-- Best results\n",
    "naive_data='./assets/pure_view_consistency_samples.png'             ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency.png'                      ;num_samples=16\n",
    "naive_data='./assets/tex_view_consistency_v0.0.4.png'               ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency_v0.0.4_tex.png'           ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency_v0.0.5.png'               ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency_v0.0.5_tex.png'           ;num_samples=32\n",
    "naive_data='./assets/high_res_tex_attempt.png'                      ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency_v0.0.5_normalized.png'    ;num_samples=32\n",
    "naive_data='./assets/tex_view_consistency_v0.0.7.png'               ;num_samples=32\n",
    "# naive_data='./assets/tex_view_consistency_v0.0.7_tex.png'           ;num_samples=32\n",
    "\n",
    "\n",
    "naive_data=load_image(naive_data)\n",
    "naive_data=as_rgb_image(as_float_image(naive_data))\n",
    "# Note: This naiva data is loaded from a png with 1 byte per color channel,\n",
    "# so it's UV values are rounded into 256 positions \n",
    "# Note: There are only only a few samples. That's ok - this isn't a dataset. It's the results of an image-to-image\n",
    "# translation algorithm, that's naive to the semantics of what the U,V values mean. In other words, that\n",
    "# simple image-to-image translation algorithm is naive to the 3d information about the cube and table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_label=77 #Some arbitrary unused label value: this is to get rid of the streaks\n",
    "\n",
    "def get_current_images():\n",
    "    #Used when I'm lazy\n",
    "    ans = [\n",
    "        \"./translator/trained_models/outputs/alphablock_without_ssim_256/images/gen_a2b_train_current.png\",\n",
    "        \"./translator/trained_models/outputs/alphablock_without_ssim_256/images/gen_a2b_test_current.png\",\n",
    "    ]; label_values = [junk_label,0,255]\n",
    "    ans = [\n",
    "        \"./translator/trained_models/outputs/config/images/gen_a2b_train_current.png\",\n",
    "        \"./translator/trained_models/outputs/config/images/gen_a2b_test_current.png\",\n",
    "    ]; label_values = [junk_label,0,255]\n",
    "    ans = [\n",
    "        \"./translator/trained_models/outputs/alphadew/images/gen_a2b_train_current.png\",\n",
    "        \"./translator/trained_models/outputs/alphadew/images/gen_a2b_test_current.png\",\n",
    "    ]; label_values = [junk_label,0,255]\n",
    "    ans = [\n",
    "        \"./translator/trained_models/outputs/five_items/images/gen_a2b_train_current.png\",\n",
    "        \"./translator/trained_models/outputs/five_items/images/gen_a2b_test_current.png\",\n",
    "    ]; label_values = [junk_label,0,50,100,150,200,255]\n",
    "    ans=load_images(ans)\n",
    "    ans=horizontally_concatenated_images(ans)\n",
    "    ans=split_tensor_into_regions(ans,7)\n",
    "    ans=ans[0],ans[4]\n",
    "    ans=vertically_concatenated_images(ans)\n",
    "    ans=as_rgb_image(as_float_image(ans))\n",
    "    return label_values, ans\n",
    "\n",
    "label_values,naive_data=get_current_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive UV/Translations:')\n",
    "display_image(naive_data)\n",
    "\n",
    "naive_scene_uv_and_labels, naive_scene_tranlations = split_tensor_into_regions(naive_data, 2 , num_samples, flat=False)\n",
    "\n",
    "icecream.ic(naive_data.min(), naive_data.max(), naive_scene_uv_and_labels.shape, naive_scene_tranlations.shape);\n",
    "\n",
    "print(\"Four random image translation results, zoomed in\")\n",
    "display_images(random_batch(naive_scene_tranlations,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_naive_scene_uv_and_labels = as_torch_images(naive_scene_uv_and_labels).to(device)\n",
    "torch_naive_scene_tranlations   = as_torch_images(naive_scene_tranlations  ).to(device)\n",
    "torch_naive_scene_uvs, torch_naive_scene_labels = extract_scene_uvs_and_scene_labels(torch_naive_scene_uv_and_labels,\n",
    "                                                                                     label_values=label_values)\n",
    "\n",
    "icecream.ic(torch_naive_scene_tranlations.shape,torch_naive_scene_tranlations.min(),torch_naive_scene_tranlations.max(),\n",
    "            torch_naive_scene_uvs        .shape,torch_naive_scene_uvs        .min(),torch_naive_scene_uvs        .max(),\n",
    "            torch_naive_scene_labels     .shape,torch_naive_scene_labels     .min(),torch_naive_scene_labels     .max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recovery_resolution=1024  #Try out different recovery resolutions! You'll see why it's sometimes best to leave it small.\n",
    "recovery_resolution=512\n",
    "recovery_resolution=256\n",
    "# recovery_resolution=128\n",
    "number_of_naive_samples=1 #As this number increases, it will become blurrier but get more coverage\n",
    "number_of_naive_samples=4 \n",
    "number_of_naive_samples=32 \n",
    "recovered_textures, recovered_weights = unproject_translations(torch_naive_scene_tranlations[:number_of_naive_samples],\n",
    "                                                               torch_naive_scene_uvs        [:number_of_naive_samples],\n",
    "                                                               torch_naive_scene_labels     [:number_of_naive_samples],\n",
    "                                                               num_labels   =len(label_values)                        ,\n",
    "                                                               output_height=recovery_resolution                      ,\n",
    "                                                               output_width =recovery_resolution                      )\n",
    "\n",
    "display_images(recovered_textures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Speculation Note: \"Blurryness\"** \n",
    "\n",
    "A question: Why are the textures so blurry? My guess: Look a bit closer on the floor texture. There's absolutely no reason this should be blurry - the image translation algorithm gets the table right almost perfectly (because it's a non-moving object that's in exactly the same place in every image). Take a look at naive_data, and you'll see the table is a lot more crisp. In particular, however, it seems that it's been shifted only up and down: a vertical blur. First of all, this blur is becasue of the averaging operation in the unprojection function: it aggregates all textures extracted from all scenes. This means that some of the tables appeared shifted left and right randomly in the image translations relative to the UV inputs. \n",
    "\n",
    "I suspect this has something to do with the data augmentation used during the naive image translation training. I'll have to look into this more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note: \"Four Pixels\"** \n",
    "\n",
    "Observe that the dots scattered around aren't 1 pixel wide - they're four pixels wide. This is because of the calculate_subpixel_weights(...)'s usage in the unprojection function. This makes visible areas of textures more likely to collide from different views, making a better view consistency loss down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_reprojections=project_textures(torch_naive_scene_uvs, torch_naive_scene_labels, recovered_textures)\n",
    "\n",
    "naive_reprojections=as_numpy_images(naive_reprojections)\n",
    "\n",
    "print(\"Naive reprojections:\")\n",
    "display_images(naive_reprojections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_to_big_reprojections=project_textures(scene_uvs, scene_labels+1, recovered_textures)\n",
    "naive_to_big_reprojections=as_numpy_images(naive_to_big_reprojections)\n",
    "\n",
    "print(\"Naive Reprojections:\")\n",
    "display_images(naive_to_big_reprojections[:4])\n",
    "display_image_slideshow(resize_images(naive_to_big_reprojections,1/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation_path='~/CleanCode/Datasets/diff_rendering/alphabetcube_L/UV_Animation_Smooth/Anim1'\n",
    "animation_path=get_absolute_path(animation_path)\n",
    "animation_scenes=get_all_files(animation_path,sort_by='number')\n",
    "animation_scenes=load_images(animation_scenes,use_cache=True)\n",
    "animation_scenes=as_torch_images(as_numpy_array(animation_scenes)).to(device)\n",
    "animation_uvs, animation_labels=extract_scene_uvs_and_scene_labels(animation_scenes,[junk_label,0,255])\n",
    "\n",
    "\n",
    "animation_reprojections=project_textures(animation_uvs, animation_labels, recovered_textures)\n",
    "animation_reprojections=as_numpy_images(animation_reprojections)\n",
    "\n",
    "print(\"Animation:\")\n",
    "display_image_slideshow(resize_images(animation_reprojections,1/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation_path='~/CleanCode/Datasets/diff_rendering/alphabetcube_L/UV_Animation_Smooth/Anim2'\n",
    "animation_path=get_absolute_path(animation_path)\n",
    "animation_scenes=get_all_files(animation_path,sort_by='number')\n",
    "animation_scenes=load_images(animation_scenes,use_cache=True)\n",
    "animation_scenes=as_torch_images(as_numpy_array(animation_scenes)).to(device)\n",
    "animation_uvs, animation_labels=extract_scene_uvs_and_scene_labels(animation_scenes,[junk_label,0,255])\n",
    "\n",
    "\n",
    "animation_reprojections=project_textures(animation_uvs, animation_labels, recovered_textures)\n",
    "animation_reprojections=as_numpy_images(animation_reprojections)\n",
    "\n",
    "print(\"Animation:\")\n",
    "display_image_slideshow(resize_images(animation_reprojections,1/1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Demo 3: Individual Unprojections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_resolution = 256\n",
    "recovered_texture_packs, recovered_weight_packs = unproject_translations_individually(torch_naive_scene_tranlations    ,\n",
    "                                                                                      torch_naive_scene_uvs            ,\n",
    "                                                                                      torch_naive_scene_labels         ,\n",
    "                                                                                      num_labels   =len(label_values)  ,\n",
    "                                                                                      output_height=recovery_resolution,\n",
    "                                                                                      output_width =recovery_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individually recovered junk textures:\")\n",
    "display_images(recovered_texture_packs[:,0])\n",
    "\n",
    "print(\"Individually recovered cube textures:\")\n",
    "display_images(recovered_texture_packs[:,1])\n",
    "\n",
    "print(\"Individually recovered table textures:\")\n",
    "display_images(recovered_texture_packs[:,2])\n",
    "\n",
    "print(\"Individually recovered cube textures with maximum filter (purely for visual purposes):\")\n",
    "display_image(min_filter(max_filter(tiled_images(as_numpy_images(recovered_texture_packs[:,1])),diameter=3),diameter=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some individually recovered textures, along with their source images\")\n",
    "\n",
    "num_display_samples=9\n",
    "\n",
    "display_image(grid_concatenated_images([as_numpy_images(recovered_texture_packs[:num_display_samples,0]),\n",
    "                                        as_numpy_images(recovered_texture_packs[:num_display_samples,1]),\n",
    "                                        as_numpy_images(recovered_texture_packs[:num_display_samples,2]),\n",
    "                                        naive_scene_uv_and_labels              [:num_display_samples  ] ,\n",
    "                                        naive_scene_tranlations                [:num_display_samples  ] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Speculation Note: \"Streaks\"**\n",
    "\n",
    "The strange streaks appearing in the 'junk' textures from the top left corner are a result of the smooth edges around the black circles in the UV maps, and the other distortions are from the cube/table boundaries in the UV parts of the naive_data images. Because the edges aren't crisp, there's a blend between the UV's of the table and the cube, resulting in UV values that don't actually exist, and don't correspond to any object. Because they're just on the edges of objects, I suspect they'll have minimal impact on the total image texture. \n",
    "\n",
    "In addition, you might ask: \"Ok, that makes sense, but the blue values should also be interpolated; resulting in a label value that also doesn't exist. Shouldn't that mean it skips those points?\" And the answer is that it doesn't skip them but defaults to label \\#0: the junk texture. That might be fixed in the future, but for now I believe that this won't happen in the actual use-case this unprojector will be used in: as a data preprocessor for the image-to-image translation algorithm. Right now, the defualt label when we don't know what label to give is 0. That's why all the artifacts are in the top texture image (texture number zero).\n",
    "\n",
    "This all can (and will) be fixed by using better UV maps: crisp ones with no antialiasing. However, for the purposes of this tutorial, it doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note: \"Non-Blurryness\"**\n",
    "\n",
    "This note is to help with a previous speculation note, \"Blurryness\". Note how since now we're only recovering textures from an individual scene, the table is no longer blurry. The table only becomes blurry when we're recovering a single texture from multiple scenes at once. Also notice how the cube's texture isn't as blurry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# View Consistency Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Weighted Mean/Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.view_consistency import weighted_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance of the individually recovered junk textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,0],recovered_weight_packs[:,0]))))\n",
    "\n",
    "print(\"Variance of the individually recovered cube textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,1],recovered_weight_packs[:,1]))))\n",
    "\n",
    "print(\"Variance of the individually recovered table textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,2],recovered_weight_packs[:,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're displaying the variance of the first two sets of images displayed in Unprojection Demo 3. Note how there's a lot of disagreement about where the striped tape should be (which is why it's so blurry). Because of this, those areas have a high variance. \n",
    "\n",
    "This variance will be used as a *\"View Consistency Loss\"*: the neural network and neural texture will be learned to try and minimize this view inconsistency (measured by variance in recovered textures). In other words, we want to make the above and below pictures dimmer.\n",
    "\n",
    "Also, note that I'm using the variance instead of the standard deviation. That might change in the future; it really depends on what kind of results I get. I'm note sure what the best loss is - but I suspect variance is kinda like MSE from the mean, so maybe it will make a good loss function. I'll probably end up trying both losses though.\n",
    "\n",
    "For comparison, I'll show what the standard deviation looks like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard Deviation of the individually recovered junk textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,0],recovered_weight_packs[:,0])**.5)))\n",
    "\n",
    "print(\"Standard Deviation of the individually recovered cube textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,1],recovered_weight_packs[:,1])**.5)))\n",
    "\n",
    "print(\"Standard Deviation of the individually recovered table textures:\")\n",
    "display_image(full_range(as_numpy_image(weighted_variance(recovered_texture_packs[:,2],recovered_weight_packs[:,2])**.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
