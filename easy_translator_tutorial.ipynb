{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bbcef5-4358-46b6-9c19-155c9bace134",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98261340-20d7-48df-bbab-d9ca3119ce6e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb523b0e-9b01-438c-bc86-986d082ac929",
   "metadata": {
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import icecream\n",
    "import torch\n",
    "import rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc48c6-3f4e-4cb9-8deb-5b1f2d5fbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.pip_import('lpips') # https://pypi.org/project/lpips/\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84a330-6a18-4f78-84ba-fb5210f514fb",
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QDs4Im9WTQoy",
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "sys.path.append('./translator')\n",
    "from translator.easy_translator import EasyTranslator\n",
    "from translator.pytorch_msssim import numpy_msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb7ce7-1dd5-4cb4-8d43-2ef4acb13d08",
   "metadata": {
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa85a3-b966-4ce5-8c1f-90e59f7e6b72",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09caf-f560-481e-82f0-881dee44f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# devuce = 'cpu'\n",
    "torch.cuda.set_device(0) #Choose a free GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c206b5-ac5c-43af-8f62-800da87b35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c90e85-0b30-463b-ab23-f465fc5b6678",
   "metadata": {},
   "source": [
    "# Load Trainer/Data/Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff9c00-84bd-44e1-87ed-a781107cb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_ONE_NAME='three_synth_base'\n",
    "VERSION_TWO_NAME='three_synth_base__no_texture'\n",
    "\n",
    "VERSION_ONE_NAME='three_synth_base_256'\n",
    "VERSION_TWO_NAME='three_synth_base__no_texture_256'\n",
    "\n",
    "VERSION_ONE_NAME='three_synth_base_512'\n",
    "VERSION_TWO_NAME='three_synth_base__no_texture_512'\n",
    "\n",
    "label_values = [0,75,150,255]\n",
    "\n",
    "scene_folder_path_one = './datasets/three_synth/scenes_anim_steveflipped'\n",
    "scene_folder_path_two = './datasets/three_synth/scenes_anim'\n",
    "photo_folder_path     = './datasets/three_synth/photos_anim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cda8a-551b-42ad-88f8-da4479daeaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translator(version_name):\n",
    "    checkpoint_folder = './translator/trained_models/outputs/%s/checkpoints'%version_name\n",
    "    config_file       = './translator/configs/%s.yaml'%version_name\n",
    "    \n",
    "    return EasyTranslator(label_values, checkpoint_folder, config_file, device)\n",
    "\n",
    "#Since these are in inference mode, they shouldn't take much VRAM - we can have two at once\n",
    "translator_one = get_translator(VERSION_ONE_NAME)\n",
    "translator_two = get_translator(VERSION_TWO_NAME)\n",
    "\n",
    "#Does this make it faster when running multiple times?\n",
    "translator_one.translate = rp.memoized(translator_one.translate)\n",
    "translator_two.translate = rp.memoized(translator_two.translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e224c02-e27a-4728-ad62-1740ed0bc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_images_one = rp.ImageDataset(scene_folder_path_one)\n",
    "scene_images_two = rp.ImageDataset(scene_folder_path_two)\n",
    "photo_images     = rp.ImageDataset(photo_folder_path    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dcc32-3b2b-468c-ae47-87c4b5ecd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "icecream.ic(\n",
    "    len(scene_images_one),\n",
    "    len(scene_images_two),\n",
    "    len(photo_images    ),\n",
    ")\n",
    "\n",
    "assert len(scene_images_one) == len(scene_images_two) == len(photo_images)\n",
    "\n",
    "length = len(photo_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882a3e1-c738-4367-b1ba-21d37175c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_alex = lpips.LPIPS(net='alex')\n",
    "def perceptual_loss(img1, img2):\n",
    "    img1 = rp.as_float_image(rp.as_rgb_image(img1))\n",
    "    img2 = rp.as_float_image(rp.as_rgb_image(img2))\n",
    "    \n",
    "    img1 = img1*2-1 # [0,1] -> [-1,1]\n",
    "    img2 = img2*2-1 # [0,1] -> [-1,1]\n",
    "    \n",
    "    img1 = rp.as_torch_image(img1)[None]\n",
    "    img2 = rp.as_torch_image(img2)[None]\n",
    "    \n",
    "    return float(loss_fn_alex(img1, img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a9071-b4a1-417d-b26a-4fe4ccad3290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self, index):\n",
    "        \n",
    "        scene_image_one = rp.as_float_image(scene_images_one[index])\n",
    "        scene_image_two = rp.as_float_image(scene_images_two[index])\n",
    "        photo_image     = rp.as_float_image(photo_images    [index])\n",
    "\n",
    "        translation_one = translator_one.translate(scene_image_one) \n",
    "        translation_two = translator_two.translate(scene_image_two) \n",
    "\n",
    "        photo_image = translator_one.scaled_input(photo_image)\n",
    "        scene_image_one = translator_one.scaled_input(scene_image_one)\n",
    "        scene_image_two = translator_two.scaled_input(scene_image_two)\n",
    "        \n",
    "        mask = translator_one.scaled_input(scene_image_one)\n",
    "        mask = rp.as_float_image(mask)\n",
    "        mask = scene_image_one[:,:,2]<.99 #White everywhere except the table\n",
    "        # mask = mask | True # Uncomment this line to disable the mask\n",
    "\n",
    "        translation_one *= mask[:,:,None]\n",
    "        translation_two *= mask[:,:,None]\n",
    "        photo_image     *= mask[:,:,None]\n",
    "\n",
    "        l1_loss_one = abs(photo_image-translation_one).mean()\n",
    "        l1_loss_two = abs(photo_image-translation_two).mean()\n",
    "\n",
    "        msssim_one = numpy_msssim(photo_image,translation_one,normalize=True)\n",
    "        msssim_two = numpy_msssim(photo_image,translation_two,normalize=True)\n",
    "        \n",
    "        lpips_one = perceptual_loss(photo_image,translation_one)\n",
    "        lpips_two = perceptual_loss(photo_image,translation_two)\n",
    "\n",
    "        LOSS_BRIGHTNESS = 2 #How much do we multiply the loss by in the images?\n",
    "\n",
    "        def indicator(boolean):\n",
    "            #Puts a * next to the better metric\n",
    "            return '*' if boolean else ' '\n",
    "\n",
    "        output_frame = (\n",
    "            rp.labeled_image(\n",
    "                rp.grid_concatenated_images(\n",
    "                    [\n",
    "                        [\n",
    "                            rp.resize_image_to_fit(\n",
    "                                rp.cv_text_to_image(\n",
    "                                    (\n",
    "                                        (\n",
    "                                            \"Index: %i\" + '\\n'+\\\n",
    "                                                                '\\n'+\\\n",
    "                                            \"L1 Loss:   \"     + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\" + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\" + '\\n'+\\\n",
    "                                                                '\\n'+\\\n",
    "                                            \"MSSSIM:   \"      + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\" + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\" + '\\n'+\\\n",
    "                                                                '\\n'+\\\n",
    "                                            \"LPIPS:   \"       + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\" + '\\n'+\\\n",
    "                                            \"   %s %.5f : %s\"    \n",
    "                                        ) % (\n",
    "                                            index,\n",
    "                                            indicator(l1_loss_one < l1_loss_two), l1_loss_one, VERSION_ONE_NAME,\n",
    "                                            indicator(l1_loss_two < l1_loss_one), l1_loss_two, VERSION_TWO_NAME,\n",
    "                                            indicator(msssim_one  > msssim_two ), msssim_one , VERSION_ONE_NAME,\n",
    "                                            indicator(msssim_two  > msssim_one ), msssim_two , VERSION_TWO_NAME,\n",
    "                                            indicator(lpips_one   < lpips_two  ), lpips_one  , VERSION_ONE_NAME,\n",
    "                                            indicator(lpips_two   < lpips_one  ), lpips_two  , VERSION_TWO_NAME,\n",
    "                                        )\n",
    "                                    ),\n",
    "                                    scale=1,\n",
    "                                ),\n",
    "                                *rp.get_image_dimensions(photo_image),\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                photo_image,\n",
    "                                'Ground Truth',\n",
    "                                size=20,\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                mask,\n",
    "                                'Mask',\n",
    "                                size=20,\n",
    "                            ),\n",
    "                        ],\n",
    "                        [\n",
    "                            rp.labeled_image(\n",
    "                                scene_image_one, \n",
    "                                'Untranslated UVL Scene',\n",
    "                                size=20,\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                translation_one,\n",
    "                                VERSION_ONE_NAME,\n",
    "                                size=20,\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                rp.as_grayscale_image(abs(photo_image-translation_one))*LOSS_BRIGHTNESS,\n",
    "                                'Ground Truth VS '+VERSION_ONE_NAME,\n",
    "                                size=20,\n",
    "                            ),\n",
    "                        ],\n",
    "                        [\n",
    "                            rp.labeled_image(\n",
    "                                scene_image_two, \n",
    "                                'Untranslated UVL Scene',\n",
    "                                size=20,\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                translation_two,\n",
    "                                VERSION_TWO_NAME,\n",
    "                                size=20,\n",
    "                            ),\n",
    "                            rp.labeled_image(\n",
    "                                rp.as_grayscale_image(abs(photo_image-translation_two))*LOSS_BRIGHTNESS,\n",
    "                                'Ground Truth VS '+VERSION_TWO_NAME,\n",
    "                                size=20,\n",
    "                            ),\n",
    "                        ],\n",
    "                    ]\n",
    "                ),\n",
    "                'Translation Comparisons',\n",
    "                size=50,\n",
    "                text_color=(255,128,255),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.index           = index\n",
    "        self.scene_image_one = scene_image_one\n",
    "        self.scene_image_two = scene_image_two\n",
    "        self.photo_image     = photo_image\n",
    "        self.mask            = mask\n",
    "        self.l1_loss_one     = l1_loss_one\n",
    "        self.l1_loss_two     = l1_loss_two\n",
    "        self.msssim_one      = msssim_one\n",
    "        self.msssim_two      = msssim_two\n",
    "        self.lpips_one       = lpips_one\n",
    "        self.lpips_two       = lpips_two\n",
    "        self.output_frame    = output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef68812-80a5-454d-bec7-8df295637f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = rp.random_index(length)\n",
    "rp.display_image(Result(index).output_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206c682-8c2a-48ba-b2ea-acac09477387",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path   = 'untracked/eval_megavideo__%s__vs__%s.mp4' % (VERSION_ONE_NAME, VERSION_TWO_NAME)\n",
    "output_video_writer = rp.VideoWriterMP4(output_video_path, video_bitrate='max')\n",
    "\n",
    "display_eta = rp.eta(length, title='Writing to %s:'%output_video_path)\n",
    "\n",
    "l1_loss_one_vals = []\n",
    "l1_loss_two_vals = []\n",
    "msssim_one_vals  = []\n",
    "msssim_two_vals  = []\n",
    "lpips_one_vals   = []\n",
    "lpips_two_vals   = []\n",
    "\n",
    "for index in range(length)[::5]:\n",
    "    # display_eta(index)\n",
    "    \n",
    "    result = Result(index)\n",
    "\n",
    "    l1_loss_one_vals.append(result.l1_loss_one)\n",
    "    l1_loss_two_vals.append(result.l1_loss_two)\n",
    "    msssim_one_vals .append(result.msssim_one )\n",
    "    msssim_two_vals .append(result.msssim_two )\n",
    "    lpips_one_vals  .append(result.lpips_one  )\n",
    "    lpips_two_vals  .append(result.lpips_two  )\n",
    "    \n",
    "    output_video_writer.write_frame(result.output_frame)\n",
    "    \n",
    "output_video_writer.finish()\n",
    "clear_output()\n",
    "print(\"Done! Download video from\", output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf25e9-a12d-47f1-8f55-bce8d8b64421",
   "metadata": {},
   "outputs": [],
   "source": [
    "icecream.ic(\n",
    "    rp.mean(l1_loss_one_vals),\n",
    "    rp.mean(l1_loss_two_vals),\n",
    "    rp.mean(msssim_one_vals ),\n",
    "    rp.mean(msssim_two_vals ),\n",
    "    rp.mean(lpips_one_vals  ),\n",
    "    rp.mean(lpips_two_vals  ),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffdcc0f-f034-40a0-9e7f-c933a4e72bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show some graphs of the losses over the frames of the video\n",
    "\n",
    "rp.line_graph_via_bokeh(\n",
    "    dict(\n",
    "        l1_loss_one = l1_loss_one_vals,\n",
    "        l1_loss_two = l1_loss_two_vals,\n",
    "    ),\n",
    "    title = 'L1 Loss (Lower is better)',\n",
    "    xlabel = 'Frame Number',\n",
    "    ylabel = 'Loss',\n",
    "    logy=10,\n",
    ")\n",
    "\n",
    "rp.line_graph_via_bokeh(\n",
    "    dict(\n",
    "        msssim_one  = msssim_one_vals ,\n",
    "        msssim_two  = msssim_two_vals ,\n",
    "    ),\n",
    "    title = 'MSSSIM (Multiscale Structural Image Similarity - Higher is better)',\n",
    "    xlabel = 'Frame Number',\n",
    "    ylabel = 'Loss',\n",
    "    logy=10,\n",
    ")\n",
    "\n",
    "rp.line_graph_via_bokeh(\n",
    "    dict(\n",
    "        lpips_one   = lpips_one_vals  ,\n",
    "        lpips_two   = lpips_two_vals  ,\n",
    "    ),\n",
    "    title = 'LPIPS (Perceptual Loss - Lower is better)',\n",
    "    xlabel = 'Frame Number',\n",
    "    ylabel = 'Loss',\n",
    "    logy=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71f8a3-f4a0-4d45-959d-4f7e0748a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "scenes_data": {
   "active_scene": "JustVid",
   "init_scene": null,
   "scenes": [
    "Default Scene",
    "JustVid"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
