{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bbcef5-4358-46b6-9c19-155c9bace134",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98261340-20d7-48df-bbab-d9ca3119ce6e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297479ca-2c18-4cb9-aab9-10ac37619b0d",
   "metadata": {
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import icecream\n",
    "from icecream import ic\n",
    "sys.path.append('./translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1930b7-b2eb-462f-a765-49bab7851885",
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QDs4Im9WTQoy",
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import rp\n",
    "import numpy as np\n",
    "from translator.trainer import MUNIT_Trainer as Trainer\n",
    "from translator.data import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee28c95-aa1d-4e31-8175-9cd1a473d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.unprojector import unproject_translations\n",
    "from source.unprojector import unproject_translations_individually\n",
    "from source.unprojector import combine_individual_unprojections\n",
    "from source.projector import project_textures\n",
    "from source.projector import colorized_scene_labels\n",
    "from source.scene_reader import extract_scene_uvs_and_scene_labels\n",
    "from source.color_quantizer import quantize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb7ce7-1dd5-4cb4-8d43-2ef4acb13d08",
   "metadata": {
    "scene__JustVid": true,
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa85a3-b966-4ce5-8c1f-90e59f7e6b72",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09caf-f560-481e-82f0-881dee44f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# devuce = 'cpu'\n",
    "torch.cuda.set_device(0) #Choose a free GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c206b5-ac5c-43af-8f62-800da87b35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c90e85-0b30-463b-ab23-f465fc5b6678",
   "metadata": {},
   "source": [
    "# Load Trainer/Data/Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8c826-a8c7-449d-ab9c-253907611853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Alphablock\n",
    "# config_file       = './translator/configs/alphablock_without_ssim_256.yaml'\n",
    "# image_folder_path = './datasets/alphacube/scenes/'\n",
    "# # image_folder_path = './datasets/alphacube/anim_2/'\n",
    "# # image_folder_path = './datasets/alphacube/anim_1/'\n",
    "# image_folder_path = '/mnt/Noman/Ubuntu/CleanCode/Datasets/diff_rendering/alphabetcube_L/SyntheticData/Anim3/Renderings'\n",
    "# checkpoint_folder = './translator/trained_models/outputs/alphablock_without_ssim_256/checkpoints'\n",
    "# # checkpoint_folder = './translator/trained_models/outputs/alphablock_without_ssim_256/checkpoints/old_checkpoints/v0.0.7'\n",
    "# # checkpoint_folder = './translator/save'\n",
    "# label_values = [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e54e4-7d18-445b-937f-4222b10830b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sunkist\n",
    "# config_file       = './translator/configs/config.yaml'\n",
    "# image_folder_path = '/mnt/Noman/Ubuntu/CleanCode/Datasets/diff_rendering/sunkist/synthetic/RenderingsAnim'\n",
    "# checkpoint_folder = './translator/trained_models/outputs/config/checkpoints'\n",
    "# label_values = [0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea55e0b-eb93-4860-b634-be59786f199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Alphadew\n",
    "# config_file       = './translator/configs/alphadew.yaml'\n",
    "# image_folder_path = './datasets/alphadew/scenes'\n",
    "# image_folder_path = '/home/Anonymous/CleanCode/Datasets/diff_rendering/sunkist_alphacube/synthetic/UV_Label_Exr_Anim'\n",
    "# image_folder_path = '/home/Anonymous/CleanCode/Datasets/diff_rendering/sunkist_alphacube/synthetic/UV_Label_Exr_Anim_2625'\n",
    "# # image_folder_path = '/home/Anonymous/CleanCode/Datasets/diff_rendering/sunkist_alphacube/synthetic/UV_Label_Exr_Mutant'\n",
    "# photo_folder_path = './datasets/alphadew/photos'\n",
    "# checkpoint_folder = './translator/trained_models/outputs/alphadew/checkpoints'\n",
    "# label_values = [0,127,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e63267-1969-4bb6-8d93-6fd4098c7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./translator/trained_models/outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b8d9c-0377-42c3-bd50-633af0fc1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_NAME='five_items__7_batch'\n",
    "VERSION_NAME='five_items__shotgun' #Best one so far\n",
    "# VERSION_NAME='five_items_original'\n",
    "# VERSION_NAME='five_items__no_consis_but_tex'\n",
    "VERSION_NAME='five_items__shotgun__variable_sizes' #Best one so far\n",
    "VERSION_NAME='panorama_frontyard__var2' #Best one so far\n",
    "VERSION_NAME='panorama_frontyard__var3' #Best one so far\n",
    "VERSION_NAME='panorama_frontyard' #Best one so far\n",
    "VERSION_NAME='panorama_frontyard' #Best one so far\n",
    "VERSION_NAME='base__only_b2a' #Best one so far\n",
    "# VERSION_NAME='panorama_frontyard__var3_run2' #Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa78dbe-ad67-47f5-8cfa-62f7d742ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Five Items\n",
    "# config_file       = './translator/configs/five_items.yaml'\n",
    "# # image_folder_path = '/home/Anonymous/CleanCode/Datasets/diff_rendering/sunkist_alphacube/synthetic/UV_Label_Exr_Mutant'\n",
    "# image_folder_path = './datasets/five_items/scenes'\n",
    "# image_folder_path = rp.get_absolute_path('~/CleanCode/Datasets/diff_rendering/five_items/synthetic/UV_Label_Exr_Anim')\n",
    "# # image_folder_path = '/mnt/Noman/Ubuntu/CleanCode/Datasets/diff_rendering/five_items/synthetic/UV_Label_Exr_Anim_2'\n",
    "# photo_folder_path = './datasets/five_items/photos'\n",
    "\n",
    "# checkpoint_folder = './translator/trained_models/outputs/%s/checkpoints'%VERSION_NAME\n",
    "# config_file       = './translator/configs/%s.yaml'%VERSION_NAME\n",
    "\n",
    "# label_values = [0,50,100,150,200,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d0637-bdbf-4bb7-8a5b-887bd9bebb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Panoramas\n",
    "# image_folder_path = './datasets/panoramas/scenes'\n",
    "# photo_folder_path = './datasets/panoramas/photos_frontyard'\n",
    "\n",
    "# checkpoint_folder = './translator/trained_models/outputs/%s/checkpoints'%VERSION_NAME\n",
    "# config_file       = './translator/configs/%s.yaml'%VERSION_NAME\n",
    "\n",
    "# label_values = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b3fa1-7144-4adb-a8a7-6889eab5bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three Synth Items\n",
    "VERSION_NAME='three_synth_base' #Best one so far\n",
    "VERSION_NAME='three_synth_base__no_texture' #Best one so far\n",
    "\n",
    "image_folder_path = './datasets/three_synth/scenes_anim'\n",
    "photo_folder_path = './datasets/three_synth/photos'\n",
    "\n",
    "checkpoint_folder = './translator/trained_models/outputs/%s/checkpoints'%VERSION_NAME\n",
    "config_file       = './translator/configs/%s.yaml'%VERSION_NAME\n",
    "\n",
    "label_values = [0,75,150,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d7724-789a-498d-9e88-cc027a262687",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = rp.load_dyaml_file(config_file)\n",
    "config = rp.DictReader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd994fdf-d690-4dde-99aa-dc03da8cd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(config, trainable=False).to(device)\n",
    "trainer.train();\n",
    "# device = torch.device('cpu'); ###Why is this necessary???\n",
    "# trainer=trainer.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c516e9-ffba-4de4-b039-68a7eb5a1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=trainer.resume(checkpoint_folder)\n",
    "print('Iteration:',iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71943-7f6d-47d9-8711-669e7c2a3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rp.display_image(rp.tiled_images(rp.as_numpy_images(trainer.cpu().texture_pack())))\n",
    "rp.display_image(rp.tiled_images(rp.as_numpy_images(trainer.texture_pack())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dafbc9-4b72-40b7-8a6f-0ab673649c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = {}\n",
    "aug[\"new_size_min\"] = config.new_size_min_a\n",
    "aug[\"new_size_max\"] = config.new_size_max_a\n",
    "aug[\"output_size\" ] = (-1,-1) #Meaningless when skip_crop = True\n",
    "# aug[\"output_size\" ] = (320,320) #Meaningless when skip_crop = True\n",
    "image_folder = ImageFolder(root=image_folder_path, precise=True, augmentation=aug)\n",
    "image_folder.skip_crop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f22fbb-dd12-4fda-9f7b-877ff2b6dddb",
   "metadata": {},
   "source": [
    "# Display Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859811e-840c-451f-b92e-13afa7c12aae",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9097fa8-e11e-4f13-8489-33a3d779ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=rp.random_element(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be34ae-f035-4a8b-b734-c8d363e7867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, height, width = o.shape\n",
    "icecream.ic(height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b9319-a9b0-4e01-8948-a21d1e28182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_image(rp.as_numpy_image(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f9d49-5254-4b9d-bd4b-d9860acb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=o[None].to(device)\n",
    "i=trainer.sample_a2b(h)\n",
    "\n",
    "\n",
    "rp.display_image(rp.as_numpy_image(h[0]))\n",
    "rp.display_image(rp.as_numpy_image(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a04f55-2448-4b93-8009-8878a72b66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imteron=trainer.sample(h,h)[:7]\n",
    "for x in imteron:\n",
    "    rp.display_image(rp.as_numpy_image(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319ec8f-d46b-4ce8-961c-6b672285267b",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1990ad-6527-4301-bbb3-4f78c7bfdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = rp.get_all_files(image_folder_path, sort_by='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5544fb9-ec2f-4ac4-ba1e-1736d5e29e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(image):\n",
    "    #Rescale the image to the same size it was trained on\n",
    "    return rp.cv_resize_image(image, (height,width), interp='nearest')\n",
    "\n",
    "def scale_images(images):\n",
    "    return [scale_image(x) for x in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d469bda-215d-43e6-8eac-4194afb81681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(image):\n",
    "    #Input image is a UV-L Scene\n",
    "    \n",
    "    assert rp.is_image(image)\n",
    "    \n",
    "    #Rescale the image to the same size it was trained on\n",
    "    image = scale_image(image)\n",
    "    \n",
    "    image = rp.as_rgb_image  (image)\n",
    "    image = rp.as_float_image(image)\n",
    "    \n",
    "    image = rp.as_torch_image(image)[None] #BCHW\n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = trainer.sample_a2b(image)\n",
    "    output = output[0]\n",
    "    output = rp.as_numpy_image(output)\n",
    "    \n",
    "    #Sometimes the network might change the dimensions.\n",
    "    #Make sure the output is the same size as the input.\n",
    "    output = rp.cv_resize_image(output, size=(height, width))\n",
    "    \n",
    "    return output\n",
    "\n",
    "rp.display_image(translate(rp.load_image(rp.random_element(image_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b51f8a-6c80-44d8-85d9-21ca5508c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinism test: Make sure it's not random!\n",
    "image=rp.load_image(rp.random_element(image_paths))\n",
    "trans1=translate(image)\n",
    "trans2=translate(image)\n",
    "\n",
    "print(\"This should be 0\")\n",
    "abs(trans1-trans2).max()\n",
    "\n",
    "#This is random! Where is the randomness coming from??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed6549-db8c-44e9-a959-7df02b67360d",
   "metadata": {},
   "source": [
    "# Recover Textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0e5b8-5da7-4947-a853-8e9b9c3752d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recovered_textures(num_samples=150, recovery_resolution=750):\n",
    "    #This is in a function to guarentee that all variables will be garbage-collected\n",
    "    #(It can take a lot of memory)\n",
    "    \n",
    "    samples = (\n",
    "        scale_images(\n",
    "            rp.load_images(\n",
    "                rp.random_batch(image_paths, min(len(image_paths),num_samples)),\n",
    "                show_progress=True\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    clear_output()\n",
    "    \n",
    "    scene_uvs, scene_labels = (\n",
    "        extract_scene_uvs_and_scene_labels(\n",
    "            rp.as_torch_images(\n",
    "                rp.as_numpy_array(samples)\n",
    "            ),\n",
    "            label_values\n",
    "        )\n",
    "    )\n",
    "    scene_uvs[scene_uvs>1] = 1 #Due to some bug in blender's dataset generation\n",
    "    \n",
    "    display_eta = rp.eta(num_samples, title='Translating Samples')\n",
    "    scene_trans = []\n",
    "    for i,sample in enumerate(samples):\n",
    "        display_eta(i)\n",
    "        scene_trans.append(translate(sample))\n",
    "    scene_trans = rp.as_torch_images(rp.as_numpy_array(scene_trans))\n",
    "    clear_output()\n",
    "        \n",
    "    print(\"Unprojecting Textures...\")\n",
    "    with torch.no_grad():\n",
    "        #Right now this takes a lot of ram or vram. This can be optimized.\n",
    "        recovered_textures, recovered_weights = unproject_translations(scene_trans      ,\n",
    "                                                                       scene_uvs,  scene_labels,\n",
    "                                                                       num_labels   =len(label_values)     ,\n",
    "                                                                       output_height=recovery_resolution   ,\n",
    "                                                                       output_width =recovery_resolution   ,\n",
    "                                                                       version = 'low memory')\n",
    "    clear_output()\n",
    "        \n",
    "    return recovered_textures\n",
    "\n",
    "recovered_textures = get_recovered_textures()\n",
    "rp.display_image(rp.tiled_images(rp.as_numpy_images(recovered_textures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da47ba9-f3d4-41c7-a280-f0aca8c7ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject(image):\n",
    "    #Takes a UVL image and returns a reprojected image from the reconstructed texture\n",
    "    assert rp.is_image(image)\n",
    "    \n",
    "    scene_uvs, scene_labels = (\n",
    "        extract_scene_uvs_and_scene_labels(\n",
    "            rp.as_torch_images(\n",
    "                rp.as_numpy_array([image])\n",
    "            ),\n",
    "            label_values\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    reprojections=project_textures(scene_uvs,scene_labels,recovered_textures)\n",
    "    \n",
    "    reprojection = rp.as_numpy_images(reprojections)[0]\n",
    "    \n",
    "    return reprojection\n",
    "    \n",
    "rp.display_image(reproject(rp.load_image(rp.random_element(image_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2806f-fe8c-4ec9-8fe6-49ff99c9cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shadows(translation, reprojection):\n",
    "    assert rp.is_image(translation)\n",
    "    assert rp.is_image(reprojection)\n",
    "    translation=scale_image(translation)\n",
    "    reprojection=scale_image(reprojection)\n",
    "    return (translation-reprojection)+.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f497be-b6b0-4378-a795-8bc6f1b8fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_image(sample):\n",
    "    assert rp.is_image(sample), 'sample is a UVL image'\n",
    "    sample=scale_image(sample)\n",
    "    reprojection=reproject(sample)\n",
    "    translation=translate(sample)\n",
    "    shadow =get_shadows(translation, reprojection)\n",
    "    sample,reprojection,translation,shadow = rp.labeled_images([sample,reprojection,translation,shadow], \n",
    "                                                        ['Sample','Reprojection','Translation','Shadow'])\n",
    "    return rp.grid_concatenated_images([[sample,shadow],[translation,reprojection]])\n",
    "\n",
    "sample=rp.load_image(rp.random_element(image_paths))\n",
    "rp.display_image(get_analysis_image(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89e5bd-fd1a-4fac-ad0a-f387a044a5e8",
   "metadata": {},
   "source": [
    "# Play with Reconstructed UVL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34858e5b-e911-4073-b8c7-1bc38ac6b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uvl=imteron[-1][0]\n",
    "# # uvl=imteron[-4][0]\n",
    "# rp.display_image(rp.as_numpy_image(uvl))\n",
    "# def round_labels(recovered_uvl):\n",
    "#     u,v,l=recovered_uvl\n",
    "#     l=l[None]\n",
    "#     l=quantize_image(l,torch.Tensor(label_values).to(device)[:,None]/255)\n",
    "#     return torch.stack((u,v,l[0]))\n",
    "    \n",
    "# uvl_q=round_labels(uvl)\n",
    "# pro=reproject(rp.as_numpy_image(uvl_q))\n",
    "    \n",
    "# rp.display_image(rp.as_numpy_image(round_labels(uvl)))\n",
    "# rp.display_image(rp.as_numpy_image(abs(round_labels(uvl)-uvl))*100)\n",
    "# rp.display_image(pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdbd9d-fd9f-48b7-ba9d-7327b33dcebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_uvs,scene_labels=extract_scene_uvs_and_scene_labels(uvl[None],label_values)\n",
    "tex=project_textures(scene_uvs,scene_labels,trainer.texture_pack())\n",
    "rp.display_image(rp.as_numpy_image(tex[0]))\n",
    "trans=trainer.sample_a2b(uvl[None])\n",
    "rp.display_image(rp.as_numpy_image(trans[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0673bc-6bed-46cb-aff6-df420934d546",
   "metadata": {},
   "source": [
    "# Photos to UVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8082c9d-5183-41fd-8a27-744836c30ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos=rp.load_images(rp.random_batch(rp.get_all_files(photo_folder_path),10))\n",
    "photo=rp.random_element(photos)\n",
    "photo=scale_image(photo)\n",
    "rp.display_image(photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5276c4d-f503-4ae1-9343-fe953a9d41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_photo=rp.as_torch_image(photo).to(device)[None]\n",
    "# print(photo.shape)\n",
    "# result=trainer.sample_b2a(t_photo)[0]\n",
    "# tex=result[:3]\n",
    "# uvl=result[3:]\n",
    "# uvl[2]=quantize_image(uvl[2][None].to(device),(torch.Tensor(label_values)/255)[:,None].to(device)).double()[0]\n",
    "# rp.display_image(rp.as_numpy_image(uvl))\n",
    "# rp.display_image(rp.as_numpy_image(tex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a150880-d0f3-41ab-a71b-706777787da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep=reproject(rp.as_numpy_image(uvl))\n",
    "rp.display_image(photo)\n",
    "rp.display_image(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b67711-7dad-4c7e-bfbb-bcfc3ed0fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scene_uvs,scene_labels=extract_scene_uvs_and_scene_labels(uvl[None],label_values)\n",
    "tex=project_textures(scene_uvs,scene_labels,trainer.texture_pack())\n",
    "rp.display_image(rp.as_numpy_image(tex[0]))\n",
    "trans=trainer.sample_a2b(uvl[None])\n",
    "rp.display_image(rp.as_numpy_image(trans[0]))\n",
    "rp.display_image(photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b2ece-aad5-4e60-882a-ac3f7f66df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    [0.5 , 0   , 0.5 ] ,\n",
    "    [0.5 , 1   , 0   ] ,\n",
    "    [0   , 0.5 , 0   ] ,\n",
    "    [0.5 , 0.5 , 0   ] ,\n",
    "    [0   , 1   , 1   ] ,\n",
    "    [0   , 0   , 1   ] ,\n",
    "    [0   , 0.5 , 0.5 ] ,\n",
    "    [0.5 , 1   , 0.5 ] ,\n",
    "    [0   , 0   , 0   ] ,\n",
    "    [0.5 , 0   , 0   ] ,\n",
    "    [1   , 0   , 0.5 ] ,\n",
    "    [0   , 0.5 , 1   ] ,\n",
    "    [1   , 0.5 , 0   ] ,\n",
    "    [1   , 1   , 1   ] ,\n",
    "    [0   , 1   , 0   ] ,\n",
    "    [0.5 , 0.5 , 0.5 ] ,\n",
    "    [0.5 , 0.5 , 1   ] ,\n",
    "    [1   , 1   , 0   ] ,\n",
    "    [0.5 , 1   , 1   ] ,\n",
    "    [0   , 0   , 0.5 ] ,\n",
    "    [1   , 0   , 0   ] ,\n",
    "    [0   , 1   , 0.5 ] ,\n",
    "    [0.5 , 0   , 1   ] ,\n",
    "    [1   , 0.5 , 1   ] ,\n",
    "    [1   , 0   , 1   ] ,\n",
    "    [1   , 1   , 0.5 ] ,\n",
    "    [1   , 0.5 , 0.5 ] ,\n",
    "]\n",
    "\n",
    "label_image=colorized_scene_labels(scene_labels, torch.Tensor(colors[:len(label_values)]))\n",
    "rp.display_image(rp.as_numpy_image(label_image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50e9c0-a2f1-4d80-a144-2c244f5f2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl=len(label_values)\n",
    "# rp.display_image(rp.as_numpy_array((uvl[2]*nl).floor()/nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770092a3-2026-433d-a914-189f1d2b5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl=len(label_values)-1\n",
    "# rp.display_image(rp.as_numpy_array((uvl[2]*nl).round()/nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b04eb-27f1-4fbb-a2e8-562692bef033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Specific to alphadew; correction to quantizer\n",
    "# label_rounded=(uvl[2]*nl).round().type(torch.int64)\n",
    "# lr2=label_rounded.clone()\n",
    "# lr2[lr2==1]=127\n",
    "# lr2[lr2==2]=255\n",
    "# # lr2=lr2/255\n",
    "\n",
    "# ic(label_rounded[None].shape,scene_labels.shape,scene_uvs.shape)\n",
    "# ic(label_rounded[None].dtype,scene_labels.dtype)\n",
    "\n",
    "# qwe=torch.concat((scene_uvs[0],lr2[None]/255),0)\n",
    "# qwe=rp.as_numpy_image(qwe)\n",
    "# rp.display_image(qwe)\n",
    "# rp.display_image(photo)\n",
    "# rp.display_image(reproject(qwe))\n",
    "\n",
    "# tex=project_textures(scene_uvs,label_rounded[None],trainer.texture_pack())\n",
    "# rp.display_image(rp.as_numpy_image(tex[0]))\n",
    "# trans=trainer.sample_a2b(uvl[None])\n",
    "# rp.display_image(rp.as_numpy_image(trans[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80be05-b34a-4479-ba41-bf8d1204b0a1",
   "metadata": {},
   "source": [
    "# Make a Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a8999-d311-4519-b949-b411f3e6bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_eta = rp.eta(len(image_paths),title='Translation eta')\n",
    "video_path = 'untracked/megavideo__%s__iter_%i.mp4'%(VERSION_NAME,iteration)\n",
    "\n",
    "writer_megavideo = rp.VideoWriterMP4(video_path ,video_bitrate='max')\n",
    "\n",
    "for i,image_path in enumerate(image_paths):\n",
    "    display_eta(i)\n",
    "    \n",
    "    uvl_scene = rp.load_image(image_path)\n",
    "    \n",
    "    if not np.any(uvl_scene):\n",
    "        #Blender messed up on a few frames, leaving them black...\n",
    "        continue\n",
    "        \n",
    "    analysis_image = get_analysis_image(uvl_scene)\n",
    "    writer_megavideo.write_frame(analysis_image)\n",
    "    \n",
    "writer_megavideo.finish()\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "scenes_data": {
   "active_scene": "JustVid",
   "init_scene": null,
   "scenes": [
    "Default Scene",
    "JustVid"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
